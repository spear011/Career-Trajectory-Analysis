{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6afa91-ae37-4d03-af55-c9cd633c8c01",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "\n",
    "This notebook documents the end-to-end process for constructing a cleaned, enriched career trajectories dataset from individuals' job and education records (provided by Lightcast), corresponding predicted race and gender attributes, and corresponding state-level GDPs and occupational wage data (sourced from the Bureau of Labor Statistics, or BLS).\n",
    "\n",
    "The final dataset (`trajectory_df`) is optimized for downstream regression analyses and other statistical modeling tasks.\n",
    "\n",
    "**Important Note on Reproducibility**\n",
    "\n",
    "* The data processing logic was developed using a proprietary, licensed commercial dataset (Lightcast data). Therefore, the raw data files are NOT included in this repository and cannot be publicly shared.\n",
    "* The file names for the job and education records data used in the code are placeholders. To run this notebook, users must substitute them with their own compatible raw data files.\n",
    "\n",
    "## Overview of Pipeline\n",
    "\n",
    "The pipeline proceeds through the following major steps:\n",
    "\n",
    "### 0. Load and Preprocess Raw Data\n",
    "- Load job (`job_df`) and education (`edu_df`) files.\n",
    "- Convert date columns to `datetime`.\n",
    "\n",
    "### 1. Job Records Filtering\n",
    "- Keep jobs with valid titles, company, city, state, and country.\n",
    "- Filter for valid start/end dates.\n",
    "- Sort jobs chronologically within each individual.\n",
    "\n",
    "### 2. Job Titles Filtering\n",
    "- Load SOC prediction (`occ_df`) file.\n",
    "- Replace SOC titles (ONET_2019)NAME) and codes (ONET_2019) with FewSOC-predicted labels in `occ_df`.\n",
    "\n",
    "### 3. Education Records Filtering\n",
    "- Retain only Bachelor’s and higher (BA, MA, PhD) degrees.\n",
    "- Drop records with missing fields.\n",
    "- Sort degrees chronologically per individual.\n",
    "\n",
    "### 3b. User Intersection\n",
    "- Keep only users that appear in job and education datasets.\n",
    "\n",
    "### 4. Post-Graduation Gap Filtering\n",
    "- Compute the gap between BA graduation and first job.\n",
    "- Drop users exceeding the maximum allowable gap (varies by degree level).\n",
    "\n",
    "### 5. Timeframe Filtering\n",
    "- Restrict sample to users with:\n",
    "  - First job year ≥ 1999\n",
    "  - Last job year ≤ 2022\n",
    "\n",
    "### 6. Construct Linear Career Trajectories\n",
    "- Remove overlapping jobs.\n",
    "- Sort jobs by start date (ascending) and end date (descending).\n",
    "- Truncate each trajectory to the first 5 years after BA graduation.\n",
    "\n",
    "### 7. Flatten to `trajectory_df`\n",
    "Each row represents a single individual’s career trajectory.  \n",
    "Includes:\n",
    "- First job attributes (SOC, NAICS, company, state, start year)\n",
    "- Number of job changes\n",
    "- Highest education attained\n",
    "\n",
    "### 8. Enrich with Contextual Variables\n",
    "- Encode demographics and merge demographic attributes (gender, race):\n",
    "  - Gender: `1 = Male`, `2 = Female`\n",
    "  - Race: `1 = White`, `2 = Black`, `3 = Asian`, `4 = Hispanic`\n",
    "- Estimate birth year from BA date → assign generation cohort\n",
    "- Add state GDP at first job year → assign GDP decile\n",
    "- Add state-occupation-year wages for the first job\n",
    "- Compute job change type indicators:\n",
    "  - `move_1_1`, `move_1_2`, `move_2_1`, `move_2_2`\n",
    "- Add `up_move` indicator if last job wage ≥5% higher than first job wage\n",
    "\n",
    "### 9. Final Cleaning\n",
    "- Drop `ID` and detailed SOC columns no longer needed\n",
    "- Drop rows with any missing values\n",
    "- Log-transform first job wage:\n",
    "  ```python\n",
    "  trajectory_df['log_wage_x'] = np.log(trajectory_df['annual_state_wage_x'])\n",
    "\n",
    "* Top-code `num_job_changes` at the 95th percentile to handle outliers\n",
    "* Export cleaned dataset to Parquet:\n",
    "\n",
    "  ```python\n",
    "  trajectory_df.to_parquet('../data/career_trajectories.parquet', index=False)\n",
    "  ```\n",
    "\n",
    "## Final Data Table: \n",
    "\n",
    "| Column                                         | Description                                                               |\n",
    "| ---------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| `max_edu_name`                                 | Highest degree attained within the first 5-year timeframe                 |\n",
    "| `onet_major_x`                                 | Major SOC code (2-digit) of first job                                     |\n",
    "| `naics6_major_x`                               | 2-digit NAICS sector of first job                                         |\n",
    "| `company_x`                                    | First employer company name                                               |\n",
    "| `state_x`                                      | First job state                                                           |\n",
    "| `job_start_year_x`                             | Year of first job start                                                   |\n",
    "| `num_job_changes`                              | Number of job changes in the 5-year window (top-coded at 95th percentile) |\n",
    "| `gender`                                       | Gender (1 = Male, 2 = Female)                                             |\n",
    "| `race`                                         | Race (1 = White, 2 = Black, 3 = Asian, 4 = Hispanic)                      |\n",
    "| `generation`                                   | Estimated generation cohort                                               |\n",
    "| `state_gdp_decile_x`                           | Decile of state GDP in first job year                                     |\n",
    "| `annual_state_wage_x`                          | Occupational wage for the first job (state × 6-digit occupation × year)   |\n",
    "| `log_wage_x`                                   | Log-transformed first job wage                                            |\n",
    "| `move_1_1`, `move_1_2`, `move_2_1`, `move_2_2` | Job change type indicators for Type-1, Type-2, Type-3, respectively       |\n",
    "| `up_move`                                      | Upward mobility indicator: last job wage ≥ 5% above first job wage        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376aa587-1a3a-4c20-8540-ed912556c2ec",
   "metadata": {},
   "source": [
    "# Loading Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e46f74e-1321-42c2-b29c-d350d2955478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "\n",
    "# ============================\n",
    "# 0. Load data\n",
    "# ============================\n",
    "job_filename = '../data/samples/job_1k.csv'  # Lightcast jobs data\n",
    "edu_filename = '../data/samples/edu_1k.csv'  # Lightcast educations data\n",
    "\n",
    "job_df = pd.read_csv(job_filename, encoding=\"utf-8\")\n",
    "edu_df = pd.read_csv(edu_filename, encoding=\"utf-8\")\n",
    "\n",
    "# ============================\n",
    "# 1. Ensure date columns are datetime dtype\n",
    "# ============================\n",
    "def ensure_datetime(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Convert specified columns to datetime if they are not already.\n",
    "    This avoids redundant conversion for parquet-loaded datetime columns.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns and not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Apply to relevant columns\n",
    "ensure_datetime(job_df, ['JOB_START_DATE', 'JOB_END_DATE'])\n",
    "ensure_datetime(edu_df, ['START_DATE', 'END_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efdeea-fa03-4861-80df-2b82298f98a3",
   "metadata": {},
   "source": [
    "# Pre-processing Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb2d6d0-71aa-469f-9850-ec09aa47ea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Summary =====\n",
      "Job Table: 415 unique users, 1000 rows\n",
      "Education Table: 565 unique users, 1000 rows\n",
      "\n",
      "===== Job Table Checks =====\n",
      "Missing values in required job fields:\n",
      "| Field          |   Missing Count |\n",
      "|----------------|-----------------|\n",
      "| TITLE_RAW      |              16 |\n",
      "| COMPANY_RAW    |              69 |\n",
      "| CITY_RAW       |             372 |\n",
      "| STATE_RAW      |             339 |\n",
      "| COUNTRY_RAW    |             281 |\n",
      "| JOB_START_DATE |             302 |\n",
      "Jobs with JOB_END_DATE before JOB_START_DATE: 0\n",
      "Jobs with missing start but non-missing end: 3\n",
      "Past jobs missing end date: 93\n",
      "Jobs outside 1999–2022 timeframe: 59\n",
      "IDs in job_df missing from edu_df 262\n",
      "\n",
      "===== Education Table Checks =====\n",
      "Missing values in required education fields:\n",
      "| Field         |   Missing Count |\n",
      "|---------------|-----------------|\n",
      "| EDULEVEL_NAME |             405 |\n",
      "| SCHOOL_RAW    |              45 |\n",
      "| START_DATE    |             259 |\n",
      "| END_DATE      |             216 |\n",
      "Education records with degrees below BA or invalid: 469\n",
      "Profiles with first job before BA graduation: 35\n",
      "Profiles exceeding post-graduation gap thresholds: 8\n"
     ]
    }
   ],
   "source": [
    "def run_checks():\n",
    "    global job_df, edu_df\n",
    "\n",
    "    # ============================\n",
    "    # Summary\n",
    "    # ============================\n",
    "    print(\"===== Dataset Summary =====\")\n",
    "    print(f\"Job Table: {len(job_df['ID'].unique())} unique users, {len(job_df)} rows\")\n",
    "    print(f\"Education Table: {len(edu_df['ID'].unique())} unique users, {len(edu_df)} rows\")\n",
    "    \n",
    "    # ============================\n",
    "    # Job Table Checks\n",
    "    # ============================\n",
    "    print(\"\\n===== Job Table Checks =====\")\n",
    "\n",
    "    # Missing required job fields\n",
    "    required_job_fields = ['TITLE_RAW', 'COMPANY_RAW', 'CITY_RAW', 'STATE_RAW', 'COUNTRY_RAW', 'JOB_START_DATE']\n",
    "    missing_job_fields = job_df[required_job_fields].isna().sum().reset_index()\n",
    "    missing_job_fields.columns = ['Field', 'Missing Count']\n",
    "    print(\"Missing values in required job fields:\")\n",
    "    print(tabulate(missing_job_fields, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "    # Chronological inversion (END < START when both present)\n",
    "    invalid_chronology = job_df[\n",
    "        job_df['JOB_START_DATE'].notna() &\n",
    "        job_df['JOB_END_DATE'].notna() &\n",
    "        (job_df['JOB_END_DATE'] < job_df['JOB_START_DATE'])\n",
    "    ]\n",
    "    print(\"Jobs with JOB_END_DATE before JOB_START_DATE:\", len(invalid_chronology))\n",
    "    \n",
    "    # Missing date anomalies\n",
    "    missing_start_but_end = job_df[job_df['JOB_START_DATE'].isna() & job_df['JOB_END_DATE'].notna()]\n",
    "    missing_end_but_past = job_df[~job_df['IS_CURRENT'] & job_df['JOB_END_DATE'].isna()]\n",
    "    print(\"Jobs with missing start but non-missing end:\", len(missing_start_but_end))\n",
    "    print(\"Past jobs missing end date:\", len(missing_end_but_past))\n",
    "\n",
    "    # Jobs outside 1999–2022 timeframe\n",
    "    jobs_out_of_timeframe = job_df[\n",
    "        job_df['JOB_START_DATE'].dt.year.lt(1999) |\n",
    "        job_df['JOB_START_DATE'].dt.year.gt(2022)\n",
    "    ]\n",
    "    print(\"Jobs outside 1999–2022 timeframe:\", len(jobs_out_of_timeframe))\n",
    "\n",
    "    # Cross-table ID consistency\n",
    "    ids_job = set(job_df['ID'])\n",
    "    ids_edu = set(edu_df['ID'])\n",
    "    ids_missing = ids_job - ids_edu\n",
    "    print(\"IDs in job_df missing from edu_df\", len(ids_missing))\n",
    "\n",
    "    # ============================\n",
    "    # Education Table Checks\n",
    "    # ============================\n",
    "    print(\"\\n===== Education Table Checks =====\")\n",
    "\n",
    "    # Missing required education fields\n",
    "    required_edu_fields = ['EDULEVEL_NAME', 'SCHOOL_RAW', 'START_DATE', 'END_DATE']\n",
    "    missing_edu_fields = edu_df[required_edu_fields].isna().sum().reset_index()\n",
    "    missing_edu_fields.columns = ['Field', 'Missing Count']\n",
    "    print(\"Missing values in required education fields:\")\n",
    "    print(tabulate(missing_edu_fields, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "    # Degrees filtered (BA+)\n",
    "    valid_degrees = [\"Bachelor's Degree\", \"Master's Degree\", \"Doctorate\"]\n",
    "    invalid_degrees = edu_df[~edu_df['EDULEVEL_NAME'].isin(valid_degrees)]\n",
    "    print(\"Education records with degrees below BA or invalid:\", len(invalid_degrees))\n",
    "    \n",
    "    # Graduation date < first job start date\n",
    "    first_job = job_df.groupby('ID')['JOB_START_DATE'].min().rename('FIRST_JOB_DATE')\n",
    "    ba_grad = edu_df[edu_df['EDULEVEL_NAME'] == \"Bachelor's Degree\"] \\\n",
    "                .groupby('ID')['END_DATE'].min().rename('BA_GRAD_DATE')\n",
    "    gap_check = pd.concat([first_job, ba_grad], axis=1).dropna()\n",
    "    violating_gap = gap_check[gap_check['FIRST_JOB_DATE'] < gap_check['BA_GRAD_DATE']]\n",
    "    print(\"Profiles with first job before BA graduation:\", len(violating_gap))\n",
    "    \n",
    "    # Post-grad gap thresholds\n",
    "    degree_order = {\"Bachelor's Degree\": 1, \"Master's Degree\": 2, \"Doctorate\": 3}\n",
    "    highest_degree = edu_df.groupby('ID')['EDULEVEL_NAME'].agg(\n",
    "        lambda x: max(x, key=lambda y: degree_order.get(y, 0))\n",
    "    )\n",
    "    gap_check['HIGHEST_DEGREE'] = highest_degree\n",
    "    gap_check['POST_GRAD_GAP_YEARS'] = (\n",
    "        gap_check['FIRST_JOB_DATE'] - gap_check['BA_GRAD_DATE']\n",
    "    ).dt.days / 365.25\n",
    "\n",
    "    gap_thresholds = {\"Bachelor's Degree\": 3.75, \"Master's Degree\": 5.59, \"Doctorate\": 8.25}\n",
    "    violating_threshold = gap_check[\n",
    "        gap_check['POST_GRAD_GAP_YEARS'] >\n",
    "        gap_check['HIGHEST_DEGREE'].map(gap_thresholds)\n",
    "    ]\n",
    "    print(\"Profiles exceeding post-graduation gap thresholds:\", len(violating_threshold))\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1391ae2-a85b-481b-a0b5-9595b3612eb0",
   "metadata": {},
   "source": [
    "# Steps 1 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf3fad5-defd-4557-a35c-ee63f45510ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Summary =====\n",
      "Job Table: 72 unique users, 295 rows\n",
      "Education Table: 72 unique users, 106 rows\n",
      "\n",
      "===== Job Table Checks =====\n",
      "Missing values in required job fields:\n",
      "| Field          |   Missing Count |\n",
      "|----------------|-----------------|\n",
      "| TITLE_RAW      |               0 |\n",
      "| COMPANY_RAW    |               0 |\n",
      "| CITY_RAW       |               0 |\n",
      "| STATE_RAW      |               0 |\n",
      "| COUNTRY_RAW    |               0 |\n",
      "| JOB_START_DATE |               0 |\n",
      "Jobs with JOB_END_DATE before JOB_START_DATE: 0\n",
      "Jobs with missing start but non-missing end: 0\n",
      "Past jobs missing end date: 0\n",
      "Jobs outside 1999–2022 timeframe: 17\n",
      "IDs in job_df missing from edu_df 0\n",
      "\n",
      "===== Education Table Checks =====\n",
      "Missing values in required education fields:\n",
      "| Field         |   Missing Count |\n",
      "|---------------|-----------------|\n",
      "| EDULEVEL_NAME |               0 |\n",
      "| SCHOOL_RAW    |               0 |\n",
      "| START_DATE    |               0 |\n",
      "| END_DATE      |               0 |\n",
      "Education records with degrees below BA or invalid: 0\n",
      "Profiles with first job before BA graduation: 27\n",
      "Profiles exceeding post-graduation gap thresholds: 14\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1. Job Records Filtering\n",
    "# ============================\n",
    "\n",
    "# Filter required fields in one go, then copy\n",
    "job_df = job_df[\n",
    "    (~job_df['TITLE_RAW'].isna()) &\n",
    "    (~job_df['COMPANY_RAW'].isna()) &\n",
    "    (~job_df['CITY_RAW'].isna()) &\n",
    "    (~job_df['STATE_RAW'].isna()) &\n",
    "    (~job_df['COUNTRY_RAW'].isna()) &\n",
    "    (\n",
    "        # Non-current jobs must have start & end\n",
    "        ((~job_df['IS_CURRENT']) & (~job_df['JOB_START_DATE'].isna()) & (~job_df['JOB_END_DATE'].isna())) |\n",
    "        # Current jobs must have start date\n",
    "        ((job_df['IS_CURRENT']) & (~job_df['JOB_START_DATE'].isna()))\n",
    "    )\n",
    "].copy()  # single strategic copy\n",
    "\n",
    "# Exclude records where end date is before start date\n",
    "mask_valid_dates = (job_df['IS_CURRENT']) | (job_df['JOB_END_DATE'] >= job_df['JOB_START_DATE'])\n",
    "job_df = job_df[mask_valid_dates]\n",
    "\n",
    "# Sort jobs by individual and chronological order\n",
    "job_df.sort_values(['ID', 'JOB_START_DATE', 'JOB_END_DATE'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# ============================\n",
    "# 2. Job Titles Filtering\n",
    "# ============================\n",
    "# Replace SOC titles (ONET_2019)NAME) and codes (ONET_2019) with FewSOC-predicted labels in `occ_df`. \n",
    "# Use \"job titlle, company name\" to look up the values.\n",
    "occ_filename = '../data/predictions_gpt-3.5-turbo.csv'  # Predicted SOC titles and codes\n",
    "occ_df = pd.read_csv(occ_filename, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Education Records Filtering\n",
    "# ============================\n",
    "\n",
    "# Filter required fields in one go, then copy\n",
    "edu_df = edu_df[\n",
    "    (~edu_df['EDUCATION_RAW'].isna()) &\n",
    "    (~edu_df['SCHOOL_RAW'].isna()) &\n",
    "    (~edu_df['START_DATE'].isna()) &\n",
    "    (~edu_df['END_DATE'].isna())\n",
    "].copy()\n",
    "\n",
    "# Keep only BA+\n",
    "valid_degrees = [\"Bachelor's Degree\", \"Master's Degree\", \"Doctorate\"]\n",
    "edu_df = edu_df[edu_df['EDULEVEL_NAME'].isin(valid_degrees)]\n",
    "\n",
    "# Sort education by individual and chronological order\n",
    "edu_df.sort_values(['ID', 'START_DATE', 'END_DATE'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# ============================\n",
    "# 3b. Keep only users that appear in job, edu, and dem tables\n",
    "# ============================\n",
    "valid_ids = (\n",
    "    job_df[['ID']].drop_duplicates()\n",
    "    .merge(edu_df[['ID']].drop_duplicates(), on='ID', how='inner')\n",
    ")['ID']\n",
    "\n",
    "job_df = job_df[job_df['ID'].isin(valid_ids)]\n",
    "edu_df = edu_df[edu_df['ID'].isin(valid_ids)]\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4e601-0db8-4850-b4c7-9aa4fbe34ff7",
   "metadata": {},
   "source": [
    "# Steps 4 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6630c2d-1d57-452b-902e-114535c47bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Summary =====\n",
      "Job Table: 19 unique users, 86 rows\n",
      "Education Table: 19 unique users, 35 rows\n",
      "\n",
      "===== Job Table Checks =====\n",
      "Missing values in required job fields:\n",
      "| Field          |   Missing Count |\n",
      "|----------------|-----------------|\n",
      "| TITLE_RAW      |               0 |\n",
      "| COMPANY_RAW    |               0 |\n",
      "| CITY_RAW       |               0 |\n",
      "| STATE_RAW      |               0 |\n",
      "| COUNTRY_RAW    |               0 |\n",
      "| JOB_START_DATE |               0 |\n",
      "Jobs with JOB_END_DATE before JOB_START_DATE: 0\n",
      "Jobs with missing start but non-missing end: 0\n",
      "Past jobs missing end date: 0\n",
      "Jobs outside 1999–2022 timeframe: 0\n",
      "IDs in job_df missing from edu_df 0\n",
      "\n",
      "===== Education Table Checks =====\n",
      "Missing values in required education fields:\n",
      "| Field         |   Missing Count |\n",
      "|---------------|-----------------|\n",
      "| EDULEVEL_NAME |               0 |\n",
      "| SCHOOL_RAW    |               0 |\n",
      "| START_DATE    |               0 |\n",
      "| END_DATE      |               0 |\n",
      "Education records with degrees below BA or invalid: 0\n",
      "Profiles with first job before BA graduation: 0\n",
      "Profiles exceeding post-graduation gap thresholds: 0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Step 4: Post-Graduation Gap Filtering\n",
    "# ---------------------------\n",
    "\n",
    "# Sort education table deterministically\n",
    "edu_df = edu_df.sort_values(['ID', 'EDULEVEL_NAME', 'END_DATE'], \n",
    "                            key=lambda col: col.map({\"Bachelor's Degree\":1, \"Master's Degree\":2, \"Doctorate\":3}) \n",
    "                                             if col.name=='EDULEVEL_NAME' else col)\n",
    "\n",
    "# Compute BA graduation date per user\n",
    "ba_df = edu_df[edu_df['EDULEVEL_NAME'] == \"Bachelor's Degree\"]\n",
    "ba_grad_dates = ba_df.groupby('ID')['END_DATE'].min().rename('BA_GRAD_DATE')\n",
    "\n",
    "# First job start date\n",
    "first_job_dates = job_df.groupby('ID')['JOB_START_DATE'].min().rename('FIRST_JOB_DATE')\n",
    "\n",
    "# Highest degree per user\n",
    "degree_order = {\"Bachelor's Degree\": 1, \"Master's Degree\": 2, \"Doctorate\": 3}\n",
    "edu_df['DEGREE_ORDER'] = edu_df['EDULEVEL_NAME'].map(degree_order)\n",
    "# Take the row with max degree order per user\n",
    "idx = edu_df.groupby('ID')['DEGREE_ORDER'].idxmax()\n",
    "highest_degree = edu_df.loc[idx, ['ID','EDULEVEL_NAME']].set_index('ID')['EDULEVEL_NAME'].rename('HIGHEST_DEGREE')\n",
    "\n",
    "# Combine into gap dataframe\n",
    "gap_df = pd.concat([ba_grad_dates, first_job_dates, highest_degree], axis=1).dropna()\n",
    "\n",
    "# Remove negative gaps (first job before BA graduation)\n",
    "gap_df = gap_df[gap_df['FIRST_JOB_DATE'] >= gap_df['BA_GRAD_DATE']]\n",
    "\n",
    "# Compute post-grad gap in years (rounded to 4 decimals)\n",
    "gap_df['POST_GRAD_GAP_YEARS'] = ((gap_df['FIRST_JOB_DATE'] - gap_df['BA_GRAD_DATE']).dt.days / 365.25).round(4)\n",
    "\n",
    "# pply maximum allowed gap thresholds according to highest degree\n",
    "gap_thresholds = {\"Bachelor's Degree\": 3.75, \"Master's Degree\": 5.59, \"Doctorate\": 8.25}\n",
    "valid_ids_gap = gap_df[gap_df['POST_GRAD_GAP_YEARS'] <= gap_df['HIGHEST_DEGREE'].map(gap_thresholds)].index\n",
    "\n",
    "# Apply filter to main tables\n",
    "job_df = job_df[job_df['ID'].isin(valid_ids_gap)].copy()\n",
    "edu_df = edu_df[edu_df['ID'].isin(valid_ids_gap)].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Timeframe filtering\n",
    "# ---------------------------\n",
    "\n",
    "start_year, end_year = 1999, 2022\n",
    "\n",
    "# Compute first and last job start dates per user\n",
    "job_start_minmax = job_df.groupby('ID')['JOB_START_DATE'].agg(['min','max'])\n",
    "\n",
    "# Keep only users whose earliest job >= 1999 and latest job <= 2022\n",
    "valid_ids_time = job_start_minmax[\n",
    "    (job_start_minmax['min'].dt.year >= start_year) &\n",
    "    (job_start_minmax['max'].dt.year <= end_year)\n",
    "].index\n",
    "\n",
    "# Filter all tables to these valid users\n",
    "job_df = job_df[job_df['ID'].isin(valid_ids_time)].copy()\n",
    "edu_df = edu_df[edu_df['ID'].isin(valid_ids_time)].copy()\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2278e-137e-4f4d-b56b-f28ce08a6726",
   "metadata": {},
   "source": [
    "# Career Trajectories Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24731747-2030-4f97-abef-2afdf112d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Career Trajectories Construction\n",
    "# ---------------------------\n",
    "\n",
    "def construct_linear_job_history():\n",
    "    \"\"\"\n",
    "    Construct linear career trajectories per user by removing overlapping jobs,\n",
    "    sorting by (start_date asc, end_date desc), and truncating to 5 years.\n",
    "    Returns linear_job_df.\n",
    "    \"\"\"\n",
    "    global job_df\n",
    "    \n",
    "    df = job_df.copy()\n",
    "    df = df.sort_values(by=['ID', 'JOB_START_DATE', 'JOB_END_DATE'], ascending=[True, True, False])\n",
    "    linear_jobs = []\n",
    "\n",
    "    for uid, group in df.groupby('ID'):\n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "\n",
    "        trajectory = [group.iloc[0]]  # always keep earliest job\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            last_job = trajectory[-1]\n",
    "            current_job = group.iloc[i]\n",
    "\n",
    "            if pd.notna(current_job['JOB_START_DATE']) and pd.notna(last_job['JOB_END_DATE']):\n",
    "                if current_job['JOB_START_DATE'] >= last_job['JOB_END_DATE']:\n",
    "                    trajectory.append(current_job)\n",
    "\n",
    "        if len(trajectory) == 0:\n",
    "            continue\n",
    "\n",
    "        traj_df = pd.DataFrame(trajectory)\n",
    "        traj_df['TRAJECTORY_ORDER'] = range(1, len(traj_df) + 1)\n",
    "\n",
    "        # Compute trajectory start and end\n",
    "        traj_start = traj_df['JOB_START_DATE'].min()\n",
    "        traj_end = traj_df['JOB_END_DATE'].max()\n",
    "        if pd.isna(traj_end):\n",
    "            traj_end = traj_df['JOB_START_DATE'].max()\n",
    "        duration_years = (traj_end - traj_start).days / 365.25\n",
    "\n",
    "        # Filter out trajectories shorter than 5 years\n",
    "        if duration_years < 5:\n",
    "            continue\n",
    "\n",
    "        # Truncate to first 5 years from trajectory start\n",
    "        cutoff_date = traj_start + pd.DateOffset(years=5)\n",
    "        traj_df = traj_df[traj_df['JOB_START_DATE'] <= cutoff_date]\n",
    "\n",
    "        linear_jobs.append(traj_df)\n",
    "\n",
    "    linear_job_df = pd.concat(linear_jobs, ignore_index=True)\n",
    "    return linear_job_df\n",
    "\n",
    "\n",
    "def flatten_to_trajectory_df():\n",
    "    \"\"\"\n",
    "    Flatten linear_job_df to trajectory_df where each user is a single row with summary info.\n",
    "    \"\"\"\n",
    "    global linear_job_df, edu_df\n",
    "    trajectory_records = []\n",
    "    degree_order = {\"Bachelor's Degree\": 1, \"Master's Degree\": 2, \"Doctorate\": 3}\n",
    "\n",
    "    for uid, group in linear_job_df.groupby('ID'):\n",
    "        group = group.sort_values(by='TRAJECTORY_ORDER')\n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "\n",
    "        first_job = group.iloc[0]\n",
    "        last_job_end = group['JOB_END_DATE'].max()\n",
    "        if pd.isna(last_job_end):\n",
    "            last_job_end = group['JOB_START_DATE'].max()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Max degree within trajectory period\n",
    "        # -----------------------------\n",
    "        edu_sub = edu_df[(edu_df['ID'] == uid) & (edu_df['END_DATE'] <= last_job_end)]\n",
    "        if len(edu_sub) > 0:\n",
    "            edu_sub = edu_sub.copy()\n",
    "            edu_sub['degree_num'] = edu_sub['EDULEVEL_NAME'].map(degree_order).fillna(0)\n",
    "            # Sort by numeric degree descending, then by END_DATE descending\n",
    "            max_row = edu_sub.sort_values(['degree_num', 'END_DATE'], ascending=[False, False]).iloc[0]\n",
    "            max_edu_name = max_row['EDULEVEL_NAME']\n",
    "        else:\n",
    "            max_edu_name = None\n",
    "\n",
    "        # -----------------------------\n",
    "        # First job attributes\n",
    "        # -----------------------------\n",
    "        onet_major_x = str(first_job['ONET_2019'])[:2] if pd.notna(first_job.get('ONET_2019')) else None\n",
    "        naics6_major_x = str(first_job['NAICS6'])[:2] if pd.notna(first_job.get('NAICS6')) else None\n",
    "        onet_detailed_x = str(first_job['ONET_2019'])[:7] if pd.notna(first_job.get('ONET_2019')) else None\n",
    "        company_x = first_job.get('COMPANY_NAME', None)\n",
    "        state_x = first_job.get('STATE_RAW', None)\n",
    "        job_start_year_x = first_job['JOB_START_DATE'].year if pd.notna(first_job['JOB_START_DATE']) else None\n",
    "\n",
    "        # Number of job changes in trajectory\n",
    "        num_job_changes = len(group) - 1\n",
    "\n",
    "        trajectory_records.append({\n",
    "            'ID': uid,\n",
    "            'max_edu_name': max_edu_name,\n",
    "            'onet_major_x': onet_major_x,\n",
    "            'onet_detailed_x': onet_detailed_x,\n",
    "            'naics6_major_x': naics6_major_x,\n",
    "            'company_x': company_x,\n",
    "            'state_x': state_x,\n",
    "            'job_start_year_x': job_start_year_x,\n",
    "            'num_job_changes': num_job_changes\n",
    "        })\n",
    "\n",
    "    trajectory_df = pd.DataFrame(trajectory_records)\n",
    "    return trajectory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca6b66f-a699-490f-8f87-4242d3439019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original job_df: 19 unique users, 86 rows\n",
      "Linear job_df: 13 unique users, 29 rows\n",
      "\n",
      "Jobs in linear_job_df missing from original job_df: 0\n",
      "Users with overlapping jobs in linear_job_df: 0\n",
      "Users where first job in linear_job_df does not match original first job: 0\n",
      "\n",
      "===== Summary =====\n",
      "Original job count per user (mean, min, max): {'mean': 4.526315789473684, 'min': 1.0, 'max': 9.0}\n",
      "Linear job count per user (mean, min, max): {'mean': 2.230769230769231, 'min': 1.0, 'max': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Verifiying line_job_df and trajectory_df\n",
    "# ----------------------------------------\n",
    "\n",
    "def run_checks_linear_jobs():\n",
    "    \"\"\"\n",
    "    Compare original job_df and linear_job_df to verify linear trajectory construction.\n",
    "    \"\"\"\n",
    "    global job_df, linear_job_df\n",
    "    \n",
    "    print(f\"Original job_df: {len(job_df['ID'].unique())} unique users, {len(job_df)} rows\")\n",
    "    print(f\"Linear job_df: {len(linear_job_df['ID'].unique())} unique users, {len(linear_job_df)} rows\\n\")\n",
    "\n",
    "    # 1. Check all linear jobs exist in original job_df\n",
    "    merged = linear_job_df.merge(job_df, on=['ID','JOB_START_DATE','JOB_END_DATE','TITLE_RAW','COMPANY_RAW','CITY_RAW','STATE_RAW','COUNTRY_RAW','IS_CURRENT'], how='left', indicator=True)\n",
    "    missing_in_original = merged[merged['_merge']=='left_only']\n",
    "    print(\"Jobs in linear_job_df missing from original job_df:\", len(missing_in_original))\n",
    "    \n",
    "    # 2. Check chronological order and no overlaps per user\n",
    "    overlap_issues = []\n",
    "    for uid, group in linear_job_df.groupby('ID'):\n",
    "        group = group.sort_values(['JOB_START_DATE','JOB_END_DATE'], ascending=[True, False]).reset_index(drop=True)\n",
    "        for i in range(1, len(group)):\n",
    "            prev_end = group.loc[i-1, 'JOB_END_DATE']\n",
    "            curr_start = group.loc[i, 'JOB_START_DATE']\n",
    "            if pd.notna(prev_end) and pd.notna(curr_start) and curr_start < prev_end:\n",
    "                overlap_issues.append((uid, i, prev_end, curr_start))\n",
    "    \n",
    "    print(\"Users with overlapping jobs in linear_job_df:\", len(set(uid for uid,_,_,_ in overlap_issues)))\n",
    "    if overlap_issues:\n",
    "        print(\"Sample overlap issues:\")\n",
    "        print(tabulate(overlap_issues[:10], headers=['ID','JobIndex','PrevEnd','CurrStart'], tablefmt='github'))\n",
    "    \n",
    "    # 3. Check first job matches original earliest job\n",
    "    first_job_mismatch = []\n",
    "    for uid, group in linear_job_df.groupby('ID'):\n",
    "        first_linear = group.loc[group['JOB_START_DATE'].idxmin()]\n",
    "        first_original = job_df[job_df['ID']==uid].sort_values('JOB_START_DATE').iloc[0]\n",
    "        if first_linear['JOB_START_DATE'] != first_original['JOB_START_DATE']:\n",
    "            first_job_mismatch.append(uid)\n",
    "    \n",
    "    print(\"Users where first job in linear_job_df does not match original first job:\", len(first_job_mismatch))\n",
    "    if first_job_mismatch:\n",
    "        print(\"Sample user IDs with first job mismatch:\", first_job_mismatch[:10])\n",
    "    \n",
    "    # 4. Summary statistics\n",
    "    print(\"\\n===== Summary =====\")\n",
    "    print(\"Original job count per user (mean, min, max):\", job_df.groupby('ID').size().agg(['mean','min','max']).to_dict())\n",
    "    print(\"Linear job count per user (mean, min, max):\", linear_job_df.groupby('ID').size().agg(['mean','min','max']).to_dict())\n",
    "\n",
    "linear_job_df = construct_linear_job_history()\n",
    "run_checks_linear_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f4ea34-4492-4cfc-91d4-25b41fa094a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original linear_job_df: 13 unique users, 29 rows\n",
      "Flattened trajectory_df: 13 rows\n",
      "\n",
      "+-----------------------------------------------------+---------+\n",
      "| Check                                               |   Count |\n",
      "+=====================================================+=========+\n",
      "| Trajectory rows with jobs missing in linear_job_df  |       0 |\n",
      "+-----------------------------------------------------+---------+\n",
      "| Trajectory rows with incorrect max_edu_name         |       0 |\n",
      "+-----------------------------------------------------+---------+\n",
      "| Trajectory rows with incorrect first job attributes |       0 |\n",
      "+-----------------------------------------------------+---------+\n",
      "| Trajectory rows with incorrect num_job_changes      |       0 |\n",
      "+-----------------------------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "def run_checks_flattened():\n",
    "    \"\"\"\n",
    "    Verify that flattened trajectory_df is consistent with linear_job_df and edu_df.\n",
    "    \"\"\"\n",
    "    global linear_job_df, trajectory_df, edu_df\n",
    "\n",
    "    print(f\"Original linear_job_df: {len(linear_job_df['ID'].unique())} unique users, {len(linear_job_df)} rows\")\n",
    "    print(f\"Flattened trajectory_df: {len(trajectory_df)} rows\\n\")\n",
    "\n",
    "    edu_violations = 0\n",
    "    first_job_violations = 0\n",
    "    job_change_violations = 0\n",
    "    missing_jobs = 0\n",
    "\n",
    "    # Prepare jobs per user\n",
    "    jobs_per_user = {uid: df.sort_values('JOB_START_DATE') for uid, df in linear_job_df.groupby('ID')}\n",
    "\n",
    "    degree_order = {\"Bachelor's Degree\": 1, \"Master's Degree\": 2, \"Doctorate\": 3}\n",
    "\n",
    "    for idx, row in trajectory_df.iterrows():\n",
    "        uid = row['ID']\n",
    "        user_jobs = jobs_per_user.get(uid)\n",
    "        if user_jobs is None or user_jobs.empty:\n",
    "            missing_jobs += 1\n",
    "            continue\n",
    "\n",
    "        # First and last job end dates for trajectory\n",
    "        first_job = user_jobs.iloc[0]\n",
    "        last_job_end = user_jobs['JOB_END_DATE'].max()\n",
    "        if pd.isna(last_job_end):\n",
    "            last_job_end = user_jobs['JOB_START_DATE'].max()\n",
    "\n",
    "        # ============================\n",
    "        # 1. Max degree within trajectory period\n",
    "        # ============================\n",
    "        user_edu = edu_df[edu_df['ID'] == uid]\n",
    "        user_edu_window = user_edu[user_edu['END_DATE'] <= last_job_end]\n",
    "\n",
    "        if not user_edu_window.empty:\n",
    "            user_edu_window = user_edu_window.copy()\n",
    "            user_edu_window['degree_num'] = user_edu_window['EDULEVEL_NAME'].map(degree_order).fillna(0)\n",
    "            # Sort by degree_num descending, then END_DATE descending\n",
    "            max_row = user_edu_window.sort_values(['degree_num', 'END_DATE'], ascending=[False, False]).iloc[0]\n",
    "            max_degree = max_row['EDULEVEL_NAME']\n",
    "            if max_degree != row['max_edu_name']:\n",
    "                edu_violations += 1\n",
    "\n",
    "        # ============================\n",
    "        # 2. First job attributes\n",
    "        # ============================\n",
    "        onet_major_first2 = str(first_job['ONET_2019'])[:2] if pd.notna(first_job.get('ONET_2019')) else None\n",
    "        naics6_major_first2 = str(first_job['NAICS6'])[:2] if pd.notna(first_job.get('NAICS6')) else None\n",
    "        state_first = first_job.get('STATE_RAW', None)\n",
    "\n",
    "        if (onet_major_first2 != row['onet_major_x'] or\n",
    "            naics6_major_first2 != row['naics6_major_x'] or\n",
    "            state_first != row['state_x']):\n",
    "            first_job_violations += 1\n",
    "\n",
    "        # ============================\n",
    "        # 3. Num job changes\n",
    "        # ============================\n",
    "        num_jobs = len(user_jobs)\n",
    "        if row['num_job_changes'] != (num_jobs - 1):\n",
    "            job_change_violations += 1\n",
    "\n",
    "    # ============================\n",
    "    # Summary Table\n",
    "    # ============================\n",
    "    summary = [\n",
    "        [\"Trajectory rows with jobs missing in linear_job_df\", missing_jobs],\n",
    "        [\"Trajectory rows with incorrect max_edu_name\", edu_violations],\n",
    "        [\"Trajectory rows with incorrect first job attributes\", first_job_violations],\n",
    "        [\"Trajectory rows with incorrect num_job_changes\", job_change_violations]\n",
    "    ]\n",
    "\n",
    "    print(tabulate(summary, headers=[\"Check\", \"Count\"], tablefmt=\"grid\"))\n",
    "\n",
    "trajectory_df = flatten_to_trajectory_df()\n",
    "run_checks_flattened()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb25fa-d22c-43aa-a6dc-f27bac4e0995",
   "metadata": {},
   "source": [
    "# Attrobite Values Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e9def4-60fe-440b-a198-072d6f265b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply demographic attribute coding ---\n",
    "# 1 = male, 2 = female\n",
    "# 1 = white, 2 = black, 3 = asian, 4 = hispanic\n",
    "def choose_race(row):\n",
    "    if row['L1'] == 1:\n",
    "        return 1\n",
    "    elif row['L2'] == 1:\n",
    "        return 2\n",
    "    elif row['L3'] == 1:\n",
    "        return 3\n",
    "    elif row['L4'] == 1:\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_gender_race(demo_csv='../data/samples/dem_1k.csv'):\n",
    "    \"\"\"\n",
    "    Add 'gender' and 'race' columns to trajectory_df based on predicted value from demo_csv\n",
    "    \"\"\"    \n",
    "    global trajectory_df\n",
    "    \n",
    "    dem_df = pd.read_csv(demo_csv, encoding=\"utf-8\")\n",
    "    dem_df['race'] = dem_df.apply(lambda x: choose_race(x), axis=1)\n",
    "    dem_df.rename(columns={'L5': 'gender'}, inplace=True)\n",
    "    dem_df = dem_df.query('~race.isna() and gender > 0 and gender < 3')[['ID', 'race', 'gender']]\n",
    "\n",
    "    trajectory_df = trajectory_df.merge(dem_df, on='ID', how='left')\n",
    "\n",
    "add_gender_race()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e216850d-c0c4-46cb-9f83-52f3acc611d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_generation_cohort():\n",
    "    \"\"\"\n",
    "    Add 'generation' column to trajectory_df based on estimated birth year\n",
    "    from Bachelor's degree end date.\n",
    "    \"\"\"\n",
    "    global trajectory_df, edu_df\n",
    "\n",
    "    # Get bachelor's graduation year for each user\n",
    "    ba_grads = edu_df[edu_df['EDULEVEL_NAME'] == \"Bachelor's Degree\"].copy()\n",
    "    ba_grads['BA_GRAD_YEAR'] = ba_grads['END_DATE'].dt.year\n",
    "\n",
    "    # Estimate birth year\n",
    "    ba_grads['BIRTH_YEAR_EST'] = ba_grads['BA_GRAD_YEAR'] - 23\n",
    "\n",
    "    # Map to generation\n",
    "    def map_generation(year):\n",
    "        if pd.isna(year):\n",
    "            return None\n",
    "        year = int(year)\n",
    "        if 1997 <= year <= 2012:\n",
    "            return 'Generation Z'\n",
    "        elif 1981 <= year <= 1996:\n",
    "            return 'Millennials'\n",
    "        elif 1965 <= year <= 1980:\n",
    "            return 'Generation X'\n",
    "        elif 1946 <= year <= 1964:\n",
    "            return 'Baby Boomers'\n",
    "        elif 1928 <= year <= 1945:\n",
    "            return 'Silent Generation'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ba_grads['generation'] = ba_grads['BIRTH_YEAR_EST'].map(map_generation)\n",
    "\n",
    "    # Merge into trajectory_df\n",
    "    trajectory_df = trajectory_df.merge(\n",
    "        ba_grads[['ID', 'generation']], on='ID', how='left'\n",
    "    )\n",
    "\n",
    "add_generation_cohort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba2dfa7-b2d3-48dc-a5d7-3325bc8ab8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    # Lowercase, strip whitespace\n",
    "    name = name.lower().strip()\n",
    "    # Replace multiple spaces with single space\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    # Remove punctuation\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    return name\n",
    "\n",
    "def add_state_gdp_decile(gdp_csv='../data/1998_2022_real_gdp_by_state.csv'):\n",
    "    global trajectory_df\n",
    "    gdp_df = pd.read_csv(gdp_csv)\n",
    "\n",
    "    # Melt to long format\n",
    "    gdp_long = gdp_df.melt(id_vars=['GeoName'], var_name='Year', value_name='GDP')\n",
    "    gdp_long = gdp_long[gdp_long['Year'].str.isdigit()]\n",
    "    gdp_long['Year'] = gdp_long['Year'].astype(int)\n",
    "\n",
    "    # Normalize state names\n",
    "    gdp_long['GeoName_norm'] = gdp_long['GeoName'].apply(normalize_state_name)\n",
    "    trajectory_df['state_x_norm'] = trajectory_df['state_x'].apply(normalize_state_name)\n",
    "\n",
    "    # Build lookup dict {(state_norm, year): GDP}\n",
    "    state_year_to_gdp = {\n",
    "        (row['GeoName_norm'], row['Year']): row['GDP']\n",
    "        for _, row in gdp_long.iterrows()\n",
    "    }\n",
    "\n",
    "    # Map GDP to each row in trajectory_df\n",
    "    def lookup_gdp(row):\n",
    "        return state_year_to_gdp.get((row['state_x_norm'], row['job_start_year_x']), None)\n",
    "\n",
    "    trajectory_df['state_gdp'] = trajectory_df.apply(lookup_gdp, axis=1)\n",
    "\n",
    "    # Compute deciles\n",
    "    trajectory_df['state_gdp_decile_x'] = pd.qcut(\n",
    "        trajectory_df['state_gdp'], 10, labels=False, duplicates='drop'\n",
    "    ) + 1\n",
    "\n",
    "    # Drop temporary normalized column\n",
    "    trajectory_df.drop(columns=['state_x_norm', 'state_gdp'], inplace=True)\n",
    "\n",
    "add_state_gdp_decile('../data/1998_2022_real_gdp_by_state.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91631232-e54b-45b3-8837-985e2ebdb192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'annual_state_wage_x', missing values: 3\n",
      "Done in 2.0336 sec\n"
     ]
    }
   ],
   "source": [
    "def add_occupational_wage(wage_csv='data/wage_interpolated_1999_2022_soc_2019.csv'):\n",
    "    global trajectory_df\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Load wage table\n",
    "    # -----------------------------\n",
    "    wage_df = pd.read_csv(wage_csv, dtype={'OCC_CODE': str, 'AREA_TITLE': str, 'year': int})\n",
    "    \n",
    "    # Normalize strings\n",
    "    wage_df['OCC_CODE'] = wage_df['OCC_CODE'].astype(str).str[:7]  # truncate to 6-digit + dash\n",
    "    wage_df['AREA_TITLE'] = wage_df['AREA_TITLE'].str.strip().str.lower()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Prepare MultiIndex Series\n",
    "    # -----------------------------\n",
    "    wage_series = wage_df.set_index(['AREA_TITLE', 'year', 'OCC_CODE'])['A_MEAN']\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Normalize trajectory_df keys\n",
    "    # -----------------------------\n",
    "    trajectory_df['state_norm'] = trajectory_df['state_x'].str.strip().str.lower()\n",
    "    trajectory_df['onet_norm'] = trajectory_df['onet_detailed_x'].astype(str).str[:7]  # truncate to match wage table\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Build MultiIndex for lookup\n",
    "    # -----------------------------\n",
    "    trajectory_index = pd.MultiIndex.from_arrays([\n",
    "        trajectory_df['state_norm'],\n",
    "        trajectory_df['job_start_year_x'],\n",
    "        trajectory_df['onet_norm']\n",
    "    ])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Vectorized lookup\n",
    "    # -----------------------------\n",
    "    trajectory_df['annual_state_wage_x'] = wage_series.reindex(trajectory_index).to_numpy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. Cleanup\n",
    "    # -----------------------------\n",
    "    trajectory_df.drop(columns=['state_norm', 'onet_norm'], inplace=True)\n",
    "\n",
    "    print(f\"Added 'annual_state_wage_x', missing values: {trajectory_df['annual_state_wage_x'].isna().sum()}\")\n",
    "\n",
    "start = time.time()\n",
    "add_occupational_wage('../data/wage_interpolated_1999_2022_soc_2019.csv')\n",
    "end = time.time()\n",
    "print(f\"Done in {end - start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5199543a-ae3c-4c53-aaac-9c6c8b53d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added job movement type indicators: move_1_1, move_1_2, move_2_1, move_2_2\n"
     ]
    }
   ],
   "source": [
    "def add_job_change_types():\n",
    "    global trajectory_df, linear_job_df\n",
    "    \n",
    "    # Initialize columns in trajectory_df\n",
    "    trajectory_df[['move_1_1', 'move_1_2', 'move_2_1', 'move_2_2']] = 0\n",
    "    \n",
    "    # Ensure jobs are sorted by trajectory\n",
    "    linear_job_df_sorted = linear_job_df.sort_values(['ID', 'TRAJECTORY_ORDER']).copy()\n",
    "    \n",
    "    # For each user\n",
    "    for uid, group in linear_job_df_sorted.groupby('ID'):\n",
    "        if len(group) < 2:\n",
    "            continue  # No job change\n",
    "        \n",
    "        # Shift columns to compare consecutive jobs\n",
    "        prev_company = group['COMPANY_NAME'].shift(0)  # current row\n",
    "        next_company = group['COMPANY_NAME'].shift(-1) # next row\n",
    "        prev_onet = group['ONET_2019'].shift(0)\n",
    "        next_onet = group['ONET_2019'].shift(-1)\n",
    "        \n",
    "        # Determine job change types\n",
    "        type_1 = ((prev_company != next_company) & (prev_onet != next_onet)).astype(int)\n",
    "        type_2 = ((prev_company == next_company) & (prev_onet != next_onet)).astype(int)\n",
    "        type_3 = ((prev_company != next_company) & (prev_onet == next_onet)).astype(int)\n",
    "        type_4 = ((prev_company == next_company) & (prev_onet == next_onet)).astype(int)\n",
    "        \n",
    "        # If user has at least one occurrence of the type, set 1 in trajectory_df\n",
    "        trajectory_df.loc[trajectory_df['ID'] == uid, 'move_1_1'] = int(type_1.sum() > 0)\n",
    "        trajectory_df.loc[trajectory_df['ID'] == uid, 'move_1_2'] = int(type_2.sum() > 0)\n",
    "        trajectory_df.loc[trajectory_df['ID'] == uid, 'move_2_1'] = int(type_3.sum() > 0)\n",
    "        trajectory_df.loc[trajectory_df['ID'] == uid, 'move_2_2'] = int(type_4.sum() > 0)\n",
    "\n",
    "    print(\"Added job movement type indicators: move_1_1, move_1_2, move_2_1, move_2_2\")\n",
    "\n",
    "add_job_change_types()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d73a6740-9d21-4fd1-a033-0a7933cf4225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "| Job Change Type              |   Count |\n",
      "+==============================+=========+\n",
      "| move_1_1 (Type 1) violations |       0 |\n",
      "+------------------------------+---------+\n",
      "| move_1_2 (Type 2) violations |       0 |\n",
      "+------------------------------+---------+\n",
      "| move_2_1 (Type 3) violations |       0 |\n",
      "+------------------------------+---------+\n",
      "| move_2_2 (Type 4) violations |       0 |\n",
      "+------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "def run_checks_job_change_types():\n",
    "    \"\"\"\n",
    "    Verify that the job change type indicators in trajectory_df are consistent\n",
    "    with linear_job_df.\n",
    "    \"\"\"\n",
    "    global linear_job_df, trajectory_df\n",
    "\n",
    "    type_1_violations = 0\n",
    "    type_2_violations = 0\n",
    "    type_3_violations = 0\n",
    "    type_4_violations = 0\n",
    "\n",
    "    # Ensure jobs sorted\n",
    "    linear_job_df_sorted = linear_job_df.sort_values(['ID', 'TRAJECTORY_ORDER']).copy()\n",
    "\n",
    "    for uid, group in linear_job_df_sorted.groupby('ID'):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        prev_company = group['COMPANY_NAME'].shift(0)\n",
    "        next_company = group['COMPANY_NAME'].shift(-1)\n",
    "        prev_onet = group['ONET_2019'].shift(0)\n",
    "        next_onet = group['ONET_2019'].shift(-1)\n",
    "\n",
    "        type_1 = ((prev_company != next_company) & (prev_onet != next_onet)).any()\n",
    "        type_2 = ((prev_company == next_company) & (prev_onet != next_onet)).any()\n",
    "        type_3 = ((prev_company != next_company) & (prev_onet == next_onet)).any()\n",
    "        type_4 = ((prev_company == next_company) & (prev_onet == next_onet)).any()\n",
    "\n",
    "        # Compare with trajectory_df\n",
    "        traj_row = trajectory_df[trajectory_df['ID'] == uid].iloc[0]\n",
    "\n",
    "        if traj_row['move_1_1'] != int(type_1):\n",
    "            type_1_violations += 1\n",
    "        if traj_row['move_1_2'] != int(type_2):\n",
    "            type_2_violations += 1\n",
    "        if traj_row['move_2_1'] != int(type_3):\n",
    "            type_3_violations += 1\n",
    "        if traj_row['move_2_2'] != int(type_4):\n",
    "            type_4_violations += 1\n",
    "\n",
    "    summary = [\n",
    "        ['move_1_1 (Type 1) violations', type_1_violations],\n",
    "        ['move_1_2 (Type 2) violations', type_2_violations],\n",
    "        ['move_2_1 (Type 3) violations', type_3_violations],\n",
    "        ['move_2_2 (Type 4) violations', type_4_violations]\n",
    "    ]\n",
    "\n",
    "    print(tabulate(summary, headers=['Job Change Type', 'Count'], tablefmt='grid'))\n",
    "\n",
    "run_checks_job_change_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f32321e-5b4e-4094-b66a-9c257c6944ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last job wage lookup: 7/13 matched (53.85%)\n",
      "Added 'up_move' (threshold=0.05). Positive cases: 3\n",
      "Done in 2.1828 sec\n"
     ]
    }
   ],
   "source": [
    "def add_up_move(wage_csv='data/wage_interpolated_1999_2022_soc_2019.csv', threshold=0.05):\n",
    "    \"\"\"\n",
    "    Add 'up_move' indicator to trajectory_df based on wage change from first to last job.\n",
    "    Up_move = 1 if last job's wage > first job's wage by threshold fraction, else 0.\n",
    "    \"\"\"\n",
    "\n",
    "    global trajectory_df, linear_job_df\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Read and prepare wage table\n",
    "    # -----------------------------\n",
    "    wage_df = pd.read_csv(wage_csv, dtype={'OCC_CODE': str, 'AREA_TITLE': str, 'year': int})\n",
    "    wage_df['OCC_CODE'] = wage_df['OCC_CODE'].astype(str)\n",
    "    wage_df['AREA_TITLE'] = wage_df['AREA_TITLE'].str.strip().str.lower()\n",
    "    # Keep only necessary columns\n",
    "    wage_df = wage_df[['AREA_TITLE', 'year', 'OCC_CODE', 'A_MEAN']]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Prepare last job info\n",
    "    # -----------------------------\n",
    "    # Normalize IDs\n",
    "    trajectory_df['ID'] = trajectory_df['ID'].astype(str).str.strip()\n",
    "    linear_job_df['ID'] = linear_job_df['ID'].astype(str).str.strip()\n",
    "\n",
    "    # Last job per user\n",
    "    last_jobs = linear_job_df.sort_values(['ID','TRAJECTORY_ORDER']).groupby('ID').last().reset_index()\n",
    "    last_jobs['state_norm'] = last_jobs['STATE_RAW'].astype(str).str.strip().str.lower()\n",
    "    # Truncate ONET to 6-digit SOC code for matching wage table\n",
    "    last_jobs['onet_norm'] = last_jobs['ONET_2019'].astype(str).str[:7]\n",
    "    last_jobs['year'] = last_jobs['JOB_START_DATE'].dt.year\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Merge with wage table (vectorized)\n",
    "    # -----------------------------\n",
    "    last_jobs = last_jobs.merge(\n",
    "        wage_df.rename(columns={'AREA_TITLE':'state_norm','OCC_CODE':'onet_norm','A_MEAN':'annual_state_wage_y'}),\n",
    "        on=['state_norm','year','onet_norm'],\n",
    "        how='left'\n",
    "    )\n",
    "    last_jobs = last_jobs[['ID','annual_state_wage_y']]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Merge last job wage into trajectory_df\n",
    "    # -----------------------------\n",
    "    trajectory_df = trajectory_df.merge(last_jobs, on='ID', how='left')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Compute up_move\n",
    "    # -----------------------------\n",
    "    trajectory_df['up_move'] = ((trajectory_df['annual_state_wage_y'] - trajectory_df['annual_state_wage_x'])\n",
    "                                / trajectory_df['annual_state_wage_x'] > threshold).astype(int)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. Drop auxiliary column\n",
    "    # -----------------------------\n",
    "    trajectory_df.drop(columns=['annual_state_wage_y'], inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7. Print stats\n",
    "    # -----------------------------\n",
    "    matched = last_jobs['annual_state_wage_y'].notna().sum()\n",
    "    total = len(last_jobs)\n",
    "    print(f\"Last job wage lookup: {matched}/{total} matched ({matched/total*100:.2f}%)\")\n",
    "    print(f\"Added 'up_move' (threshold={threshold}). Positive cases: {trajectory_df['up_move'].sum()}\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "add_up_move('../data/wage_interpolated_1999_2022_soc_2019.csv', 0.05)\n",
    "end = time.time()\n",
    "print(f\"Done in {end - start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3089d-f989-4745-9c3c-51b456fbcb0d",
   "metadata": {},
   "source": [
    "# Final Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35079bd3-40e8-4a24-bffe-9605631cca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory_df cleaned and saved to data/samples/career_trajectories.paquet. Shape: (10, 18)\n"
     ]
    }
   ],
   "source": [
    "output_filename = '../data/samples/career_trajectories.parquet'\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Drop unnecessary columns\n",
    "# -----------------------------\n",
    "drop_cols = ['ID', 'onet_detailed_x']\n",
    "trajectory_df = trajectory_df.drop(columns=[c for c in drop_cols if c in trajectory_df.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Drop rows with null values\n",
    "# -----------------------------\n",
    "trajectory_df = trajectory_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Log transform wage\n",
    "# -----------------------------\n",
    "trajectory_df['log_wage_x'] = np.log(trajectory_df['annual_state_wage_x'])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Top-code num_job_changes at 95th percentile\n",
    "# -----------------------------\n",
    "if 'num_job_changes' in trajectory_df.columns:\n",
    "    threshold = int(np.percentile(trajectory_df['num_job_changes'], 95))\n",
    "    trajectory_df.loc[trajectory_df['num_job_changes'] > threshold, 'num_job_changes'] = threshold\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Export to parquet\n",
    "# -----------------------------\n",
    "trajectory_df.to_parquet(output_filename, index=False)\n",
    "print(f\"trajectory_df cleaned and saved to {output_filename}. Shape: {trajectory_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
